{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from saga import SAGA\n",
    "from svrg import SVRG\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from torch.optim.lr_scheduler import StepLR,LambdaLR\n",
    "from helpers import *\n",
    "import time\n",
    "import math\n",
    "from resnet import ResNet18\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLUSTERS = 1\n",
    "MOMENTUM_SAGA = 0.01\n",
    "MOMENTUM_SGD = 0.9\n",
    "N_SAMPLES = 50000\n",
    "betas = (0.9,0.999)\n",
    "BATCH_SIZE = 1\n",
    "data = \"cifar10\"\n",
    "X,y,X_test,y_test,IN_DIM,OUT_DIM = get_data(\"cifar10\",3,0)\n",
    "lr = 0.0001\n",
    "n_epochs = 500000\n",
    "res_step = int(n_epochs/100) #compute loss,variance and error every step iteration /100 -> 100 values\n",
    "#res_step_var = int(n_epochs/100)\n",
    "opti = \"Adam\"\n",
    "centered = \"uncentered\"\n",
    "if (centered == \"centered\"):\n",
    "    X = X/127.5-1\n",
    "    X_test = X_test/127.5-1\n",
    "else:\n",
    "    X = X/255\n",
    "    X_test = X_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.mean())\n",
    "print(X_test.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.min())\n",
    "print(X_test.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.max())\n",
    "print(X_test.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self,out_dim):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_cluster,y_cluster,_,_,_,_ = get_data(data,1,0,greyscale = True)\n",
    "\n",
    "# def get_data_partitions():\n",
    "#     Xs = []\n",
    "#     for i in range(OUT_DIM):\n",
    "#         Xs.append(X_cluster[np.argwhere(y==i)])\n",
    "#     return tuple(Xs)\n",
    "\n",
    "# Xs = get_data_partitions()\n",
    "\n",
    "# from sklearn.cluster import KMeans\n",
    "# #number of cluster per class and list of data, each only containing a single example\n",
    "# def get_class_clusters(n_clusters,data_partitions):\n",
    "#     kmeans = []\n",
    "#     for i in range(len(data_partitions)):\n",
    "#         kmeans.append(KMeans(n_clusters = n_clusters,random_state=0).fit(data_partitions[i].squeeze()))\n",
    "#     return kmeans\n",
    "\n",
    "# class_clusters = get_class_clusters(N_CLUSTERS*4,get_data_partitions())\n",
    "\n",
    "# def get_cc_proba(class_clusters,N_CLUSTERS):\n",
    "#     c = np.zeros((OUT_DIM*N_CLUSTERS))\n",
    "#     for i in range(OUT_DIM):\n",
    "#         for d in Xs[i]:\n",
    "#             c[N_CLUSTERS*i + class_clusters[i].predict(d).item()] +=1\n",
    "#     print(c)\n",
    "#     return c/X.shape[0]\n",
    "\n",
    "# c_prob = get_cc_proba(class_clusters,N_CLUSTERS*4)\n",
    "# print(c_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_SGD = ResNet18(OUT_DIM).to(device)\n",
    "# model_SAGA = ResNet18(OUT_DIM).to(device)\n",
    "\n",
    "model_SGD_losses = []\n",
    "model_SGD_epoch_losses = []\n",
    "SGD_avg_var = []\n",
    "SGD_epoch_var = []\n",
    "model_SGD_test_e = []\n",
    "\n",
    "model_SAGA_losses = []\n",
    "model_SAGA_epoch_losses = []\n",
    "SAGA_avg_var = []\n",
    "SAGA_epoch_var = []\n",
    "model_SAGA_test_e = []\n",
    "\n",
    "# model_SAGA_2 = ResNet18(OUT_DIM).to(device)\n",
    "# model_SAGA_2_losses = []\n",
    "# model_SAGA_2_epoch_losses = []\n",
    "# SAGA_2_avg_2_var = []\n",
    "# SAGA_epoch_2_var = []\n",
    "# model_SAGA_2_test_e = []\n",
    "\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_SGD = torch.optim.Adam(model_SGD.parameters(), lr = lr, betas = betas)\n",
    "# optimizer_SAGA = SAGA(model_SAGA.parameters(), n_classes=OUT_DIM, lr = lr,\n",
    "#                           class_proba = None,momentum=MOMENTUM_SAGA,compute_var = True,betas = betas,\n",
    "#                          use_adam = True)\n",
    "#NO MOMENTUM\n",
    "# optimizer_SAGA = SAGA(model_SAGA.parameters(), n_classes=OUT_DIM, lr = lr,\n",
    "#                           class_proba = None,momentum=MOMENTUM_SAGA,compute_var = True,betas = betas,\n",
    "#                          use_adam = True)\n",
    "\n",
    "\n",
    "# model_SAGA_2 = ResNet18(OUT_DIM).to(device)\n",
    "model_SAGA_2_losses = []\n",
    "model_SAGA_2_epoch_losses = []\n",
    "SAGA_2_avg_var = []\n",
    "SAGA_2_epoch_var = []\n",
    "model_SAGA_2_test_e = []\n",
    "# optimizer_SAGA_2 = SAGA(model_SAGA_2.parameters(), n_classes=OUT_DIM*4, lr = lr,\n",
    "#                           class_proba = c_prob,momentum=MOMENTUM_SAGA,compute_var = True,betas = betas,\n",
    "#                          use_adam = True)\n",
    "\n",
    "\n",
    "model_SVRG =  ResNet18(OUT_DIM).to(device)\n",
    "model_SVRG_losses = []\n",
    "model_SVRG_epoch_losses = []\n",
    "SVRG_avg_var = []\n",
    "SVRG_epoch_var = []\n",
    "model_SVRG_test_e = []\n",
    "optimizer_SVRG = SVRG(model_SVRG.parameters(), lr = lr,compute_var = True,\n",
    "                                use_adam = True)\n",
    "\n",
    "lr_lambda = lambda epoch : 1/np.sqrt(epoch+1)\n",
    "scheduler_SGD = LambdaLR(optimizer_SGD, lr_lambda = lr_lambda)\n",
    "# scheduler_SAGA = LambdaLR(optimizer_SAGA, lr_lambda = lr_lambda)\n",
    "# scheduler_SAGA_2 = LambdaLR(optimizer_SAGA_2, lr_lambda = lr_lambda)\n",
    "scheduler_SVRG = LambdaLR(optimizer_SVRG, lr_lambda = lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error(model,X,y):\n",
    "    e = 0\n",
    "    batch_size = 200\n",
    "    for i in range(int(X.shape[0]/batch_size)):\n",
    "        pred = model.forward(torch.Tensor(X_test[i*batch_size:(i+1)*batch_size]).to(device))\n",
    "        pred = (torch.max(pred.data,1)[1]).cpu().numpy()\n",
    "        err = (y[i*batch_size:(i+1)*batch_size] != pred)\n",
    "        e += err.sum()\n",
    "    return e/X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_error(model_SAGA,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # train SGD\n",
    "# tstart = time.time()\n",
    "# total_var = 0\n",
    "# epoch_var = 0\n",
    "# total_loss = 0\n",
    "# epoch_loss = 0\n",
    "# for epoch in range(n_epochs):\n",
    "#     #epoch loss and var\n",
    "#     if ((epoch % res_step == 0) and (epoch != 0)):\n",
    "#         model_SGD_epoch_losses.append(epoch_loss/res_step)\n",
    "#         SGD_epoch_var.append(epoch_var/res_step)\n",
    "#         epoch_loss = 0\n",
    "#         epoch_var = 0\n",
    "        \n",
    "#     idx = np.random.randint(X.shape[0])\n",
    "#     inputs = torch.tensor(X[idx].reshape(1,3,32,32)).to(device)\n",
    "#     labels = torch.tensor(y, dtype=torch.long)[idx].view(1).to(device)\n",
    "#     outputs = model_SGD.forward(inputs)\n",
    "#     loss = criterion(outputs, labels)\n",
    "#     loss.backward()\n",
    "#     total_loss += loss.data.item()\n",
    "#     epoch_loss += loss.data.item()\n",
    "#     model_SGD_losses.append(total_loss/(epoch+1))\n",
    "#     optimizer_SGD.step()\n",
    "#     #running var\n",
    "#     for param_group in list(model_SGD.parameters()):\n",
    "#         tot = (param_group.grad.data**2).sum()\n",
    "#         total_var += tot\n",
    "#         epoch_var += tot\n",
    "#     SGD_avg_var.append(total_var.cpu().numpy()/(epoch+1))\n",
    "#     #test error\n",
    "#     if (epoch % res_step == 0):\n",
    "#         e = compute_error(model_SGD,X_test,y_test)\n",
    "#         model_SGD_test_e.append(e)\n",
    "#         print(\"epoch: {}, loss: {}, var : {}\".format(int(epoch/res_step),model_SGD_losses[-1],SGD_avg_var[-1]))\n",
    "# #         np.save('ResNet18_{}_Adam_running_loss.npy'.format(data),np.asarray(model_SGD_losses))\n",
    "# #         np.save('ResNet18_{}_Adam_epoch_loss.npy'.format(data),np.asarray(model_SGD_epoch_losses))\n",
    "# #         np.save('ResNet18_{}_Adam_running_var.npy'.format(data),np.asarray(SGD_avg_var))\n",
    "# #         np.save('ResNet18_{}_Adam_epoch_var.npy'.format(data),np.asarray(SGD_epoch_var))\n",
    "# #         np.save('ResNet18_{}_Adam_test_err.npy'.format(data),np.asarray(model_SGD_test_e))\n",
    "#     optimizer_SGD.zero_grad()\n",
    "#     if ((epoch % N_SAMPLES == 0) and epoch != 0):\n",
    "#         scheduler_SGD.step()   \n",
    "#         for param_group in optimizer_SGD.param_groups:\n",
    "#             print(\"lr: {}\",param_group['lr'])      \n",
    "# print('Adam Elapsed time: {:.2f}s'.format(time.time() - tstart))\n",
    "# np.save('ResNet18_{}_Adam_running_loss.npy'.format(data),np.asarray(model_SGD_losses))\n",
    "# np.save('ResNet18_{}_Adam_epoch_loss.npy'.format(data),np.asarray(model_SGD_epoch_losses))\n",
    "# np.save('ResNet18_{}_Adam_running_var.npy'.format(data),np.asarray(SGD_avg_var))\n",
    "# np.save('ResNet18_{}_Adam_epoch_var.npy'.format(data),np.asarray(SGD_epoch_var))\n",
    "# np.save('ResNet18_{}_Adam_test_err.npy'.format(data),np.asarray(model_SGD_test_e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train SAGA\n",
    "# tstart = time.time()\n",
    "# total_var = 0\n",
    "# epoch_var = 0\n",
    "# total_loss = 0\n",
    "# epoch_loss = 0\n",
    "# for epoch in range(n_epochs):\n",
    "#     #epoch loss and var\n",
    "#     if ((epoch % res_step == 0) and (epoch != 0)):\n",
    "#         model_SAGA_epoch_losses.append(epoch_loss/res_step)\n",
    "#         SAGA_epoch_var.append(epoch_var/res_step)\n",
    "#         epoch_loss = 0\n",
    "#         epoch_var = 0\n",
    "    \n",
    "#     idx = np.random.randint(X.shape[0])\n",
    "#     inputs = torch.tensor(X[idx].reshape(1,3,32,32)).to(device)\n",
    "#     labels = torch.tensor(y, dtype=torch.long)[idx].view(1).to(device)\n",
    "#     outputs = model_SAGA.forward(inputs)\n",
    "#     loss = criterion(outputs, labels)\n",
    "#     loss.backward()\n",
    "#     total_loss += loss.data.item()\n",
    "#     epoch_loss += loss.data.item()\n",
    "#     model_SAGA_losses.append(total_loss/(epoch+1))\n",
    "#     _,var = optimizer_SAGA.step(labels)\n",
    "#     total_var += var\n",
    "#     epoch_var += var\n",
    "#     SAGA_avg_var.append(total_var.cpu().numpy()/(epoch+1))\n",
    "#     if (epoch % res_step == 0):\n",
    "#         e = compute_error(model_SAGA,X_test,y_test)\n",
    "#         model_SAGA_test_e.append(e)\n",
    "#         print(\"epoch: {}, loss: {}, var : {}\".format(int(epoch/res_step),model_SAGA_losses[-1],SAGA_avg_var[-1]))\n",
    "# #         np.save('ResNet18_{}_CSAGA_Adam_running_loss.npy'.format(data),np.asarray(model_SAGA_losses))\n",
    "# #         np.save('ResNet18_{}_CSAGA_Adam_epoch_loss.npy'.format(data),np.asarray(model_SAGA_epoch_losses))\n",
    "# #         np.save('ResNet18_{}_CSAGA_Adam_running_var.npy'.format(data),np.asarray(SAGA_avg_var))\n",
    "# #         np.save('ResNet18_{}_CSAGA_Adam_epoch_var.npy'.format(data),np.asarray(SAGA_epoch_var))\n",
    "# #         np.save('ResNet18_{}_CSAGA_Adam_test_err.npy'.format(data),np.asarray(model_SAGA_test_e))\n",
    "#     optimizer_SAGA.zero_grad()\n",
    "#     if ((epoch % N_SAMPLES == 0) and epoch != 0):\n",
    "#         scheduler_SAGA.step() \n",
    "#         for param_group in optimizer_SAGA.param_groups:\n",
    "#             print(\"lr: {}\",param_group['lr'])\n",
    "# print('CSAGA Adam Elapsed time: {:.2f}s'.format(time.time() - tstart))\n",
    "# np.save('ResNet18_{}_CSAGA_Adam_no_beta_running_loss.npy'.format(data),np.asarray(model_SAGA_losses))\n",
    "# np.save('ResNet18_{}_CSAGA_Adam_no_beta_epoch_loss.npy'.format(data),np.asarray(model_SAGA_epoch_losses))\n",
    "# np.save('ResNet18_{}_CSAGA_Adam_no_beta_running_var.npy'.format(data),np.asarray(SAGA_avg_var))\n",
    "# np.save('ResNet18_{}_CSAGA_Adam_no_beta_epoch_var.npy'.format(data),np.asarray(SAGA_epoch_var))\n",
    "# np.save('ResNet18_{}_CSAGA_Adam_no_beta_test_err.npy'.format(data),np.asarray(model_SAGA_test_e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #train SAGA 2 entry per class\n",
    "# tstart = time.time()\n",
    "# total_var = 0\n",
    "# epoch_var = 0\n",
    "# total_loss = 0\n",
    "# epoch_loss = 0\n",
    "# for epoch in range(n_epochs):\n",
    "#     #epoch loss and var\n",
    "#     if ((epoch % res_step == 0) and (epoch != 0)):\n",
    "#         model_SAGA_2_epoch_losses.append(epoch_loss/res_step)\n",
    "#         SAGA_2_epoch_var.append(epoch_var/res_step)\n",
    "#         epoch_loss = 0\n",
    "#         epoch_var = 0\n",
    "    \n",
    "#     idx = np.random.randint(X.shape[0])\n",
    "#     inputs = torch.tensor(X[idx].reshape(1,3,32,32)).to(device)\n",
    "#     labels = torch.tensor(y, dtype=torch.long)[idx].view(1).to(device)\n",
    "#     label = int(labels.item())\n",
    "#     cluster = class_clusters[label].predict(X_cluster[idx].reshape(1,-1))[0]\n",
    "#     saga_idx = label*4 + cluster\n",
    "#     outputs = model_SAGA_2.forward(inputs)\n",
    "#     loss = criterion(outputs, labels)\n",
    "#     loss.backward()\n",
    "#     total_loss += loss.data.item()\n",
    "#     epoch_loss += loss.data.item()\n",
    "#     model_SAGA_2_losses.append(total_loss/(epoch+1))\n",
    "#     closure = labels\n",
    "#     _,var = optimizer_SAGA_2.step(saga_idx)\n",
    "#     total_var += var\n",
    "#     epoch_var += var\n",
    "#     SAGA_2_avg_var.append(total_var.cpu().numpy()/(epoch+1))\n",
    "#     if (epoch % res_step == 0):\n",
    "#         e = compute_error(model_SAGA_2,X_test,y_test)\n",
    "#         model_SAGA_2_test_e.append(e)\n",
    "#         print(\"epoch: {}, loss: {}, var: {}\".format(int(epoch/res_step),model_SAGA_2_losses[-1],SAGA_2_avg_var[-1]))\n",
    "# #         np.save('ResNet18_{}_CSAGA_Adam_running_loss.npy'.format(data),np.asarray(model_SAGA_losses))\n",
    "# #         np.save('ResNet18_{}_CSAGA_Adam_epoch_loss.npy'.format(data),np.asarray(model_SAGA_epoch_losses))\n",
    "# #         np.save('ResNet18_{}_CSAGA_Adam_running_var.npy'.format(data),np.asarray(SAGA_avg_var))\n",
    "# #         np.save('ResNet18_{}_CSAGA_Adam_epoch_var.npy'.format(data),np.asarray(SAGA_epoch_var))\n",
    "# #         np.save('ResNet18_{}_CSAGA_Adam_test_err.npy'.format(data),np.asarray(model_SAGA_test_e))\n",
    "#     optimizer_SAGA_2.zero_grad()\n",
    "#     if ((epoch % N_SAMPLES == 0) and epoch != 0):\n",
    "#         scheduler_SAGA_2.step() \n",
    "#         for param_group in optimizer_SAGA_2.param_groups:\n",
    "#             print(\"lr: {}\",param_group['lr'])\n",
    "# print('CSAGA Adam 2pc Elapsed time: {:.2f}s'.format(time.time() - tstart))\n",
    "# np.save('ResNet18_{}_CSAGA_Adam_2_running_loss.npy'.format(data),np.asarray(model_SAGA_2_losses))\n",
    "# np.save('ResNet18_{}_CSAGA_Adam_2_epoch_loss.npy'.format(data),np.asarray(model_SAGA_2_epoch_losses))\n",
    "# np.save('ResNet18_{}_CSAGA_Adam_2_running_var.npy'.format(data),np.asarray(SAGA_2_avg_var))\n",
    "# np.save('ResNet18_{}_CSAGA_Adam_2_epoch_var.npy'.format(data),np.asarray(SAGA_2_epoch_var))\n",
    "# np.save('ResNet18_{}_CSAGA_Adam_2_test_err.npy'.format(data),np.asarray(model_SAGA_2_test_e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train SVRG adam\n",
    "tstart = time.time()\n",
    "total_var = 0\n",
    "epoch_var = 0\n",
    "total_loss = 0\n",
    "epoch_loss = 0\n",
    "SVRG_batch_size = 250 #for snapshot\n",
    "model_snapshot =  ResNet18(OUT_DIM).to(device)\n",
    "model_snapshot_avg =  ResNet18(OUT_DIM).to(device)\n",
    "snapshot_per_epoch = 10\n",
    "for epoch in range(n_epochs):  \n",
    "    #epoch loss and variance\n",
    "    if ((epoch % res_step == 0) and (epoch != 0)):\n",
    "        model_SVRG_epoch_losses.append(epoch_loss/res_step)\n",
    "        SVRG_epoch_var.append(epoch_var/res_step)\n",
    "        epoch_loss = 0\n",
    "        epoch_var = 0\n",
    "    #snapshot \n",
    "    if (epoch % (N_SAMPLES/snapshot_per_epoch) == 0):\n",
    "#         model_snapshot = copy.deepcopy(model_SVRG)\n",
    "#         model_snapshot_avg = copy.deepcopy(model_snapshot)\n",
    "        model_snapshot.load_state_dict(model_SVRG.state_dict())\n",
    "        model_snapshot_avg.load_state_dict(model_SVRG.state_dict())\n",
    "        model_snapshot_avg.zero_grad()\n",
    "        #compute full batch for snapshot\n",
    "        for i in range(int(X.shape[0]/SVRG_batch_size)):\n",
    "            inputs = torch.from_numpy(X[i*SVRG_batch_size:(i+1)*SVRG_batch_size]).to(device)\n",
    "            labels = torch.tensor(y[i*SVRG_batch_size:(i+1)*SVRG_batch_size], dtype=torch.long).to(device)\n",
    "            outputs = model_snapshot_avg.forward(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss = loss/(int(X.shape[0]/SVRG_batch_size)) #correct value for average loss\n",
    "            loss.backward()\n",
    "        print(\"Snapshot computed at iteration {}\".format(epoch))\n",
    "        model_snapshot.zero_grad()\n",
    "        model_SVRG.zero_grad()\n",
    "            \n",
    "    idx = np.random.randint(X.shape[0])\n",
    "    inputs = torch.tensor(X[idx].reshape(1,3,32,32)).to(device)\n",
    "    labels = torch.tensor(y, dtype=torch.long)[idx].view(1).to(device)\n",
    "    #snapshot grad\n",
    "    outputs = model_snapshot.forward(inputs).view(1,-1)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    #current grad\n",
    "    outputs = model_SVRG.forward(inputs).view(1,-1)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "\n",
    "    total_loss += loss.data.item()\n",
    "    epoch_loss += loss.data.item()\n",
    "    model_SVRG_losses.append(total_loss/(epoch+1))\n",
    "    _, var = optimizer_SVRG.step(snap_grad = list(model_snapshot.parameters()),\n",
    "                                 snap_avg_grad = list(model_snapshot_avg.parameters()))\n",
    "#     print(\"Iteration: {}, loss: {}, var: {}, sample: {}\".format(epoch,loss.data.item(),var,idx))\n",
    "    total_var += var\n",
    "    epoch_var += var\n",
    "    SVRG_avg_var.append(total_var.cpu().numpy()/(epoch+1))\n",
    "\n",
    "    #compute error\n",
    "    if (epoch % res_step == 0):\n",
    "        e = compute_error(model_SVRG,X_test,y_test)\n",
    "        model_SVRG_test_e.append(e)\n",
    "        print(\"epoch: {}, loss: {}, var : {}\".format(int(epoch/res_step),model_SVRG_losses[-1],SVRG_avg_var[-1]))\n",
    "    optimizer_SVRG.zero_grad()\n",
    "    model_snapshot.zero_grad()\n",
    "    #decay lr\n",
    "    if ((epoch % N_SAMPLES == 0) and epoch != 0):\n",
    "        scheduler_SVRG.step() \n",
    "        for param_group in optimizer_SVRG.param_groups:\n",
    "            print(\"lr: {}\",param_group['lr'])\n",
    "print('SVRG Adam Elapsed time: {:.2f}s'.format(time.time() - tstart))\n",
    "np.save('ResNet18_{}_SVRG_Adam_running_loss.npy'.format(data),np.asarray(model_SAGA_2_losses))\n",
    "np.save('ResNet18_{}_SVRG_Adam_epoch_loss.npy'.format(data),np.asarray(model_SAGA_2_epoch_losses))\n",
    "np.save('ResNet18_{}_SVRG_Adam_running_var.npy'.format(data),np.asarray(SAGA_2_avg_var))\n",
    "np.save('ResNet18_{}_SVRG_Adam_epoch_var.npy'.format(data),np.asarray(SAGA_2_epoch_var))\n",
    "np.save('ResNet18_{}_SVRG_Adam_test_err.npy'.format(data),np.asarray(model_SAGA_2_test_e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_SGD_losses = np.load('ResNet18_{}_Adam_running_loss.npy'.format(data),allow_pickle = True).tolist()\n",
    "model_SGD_epoch_losses = np.load('ResNet18_{}_Adam_epoch_loss.npy'.format(data),allow_pickle = True).tolist()\n",
    "SGD_avg_var = np.load('ResNet18_{}_Adam_running_var.npy'.format(data),allow_pickle = True).tolist()\n",
    "SGD_epoch_var = np.load('ResNet18_{}_Adam_epoch_var.npy'.format(data),allow_pickle = True).tolist()\n",
    "model_SGD_test_e = np.load('ResNet18_{}_Adam_test_err.npy'.format(data),allow_pickle = True).tolist()\n",
    "\n",
    "model_SAGA_losses = np.load('ResNet18_{}_CSAGA_Adam_running_loss.npy'.format(data),allow_pickle = True).tolist()\n",
    "model_SAGA_epoch_losses = np.load('ResNet18_{}_CSAGA_Adam_epoch_loss.npy'.format(data),allow_pickle = True).tolist()\n",
    "SAGA_avg_var = np.load('ResNet18_{}_CSAGA_Adam_running_var.npy'.format(data),allow_pickle = True).tolist()\n",
    "SAGA_epoch_var = np.load('ResNet18_{}_CSAGA_Adam_epoch_var.npy'.format(data),allow_pickle = True).tolist()\n",
    "model_SAGA_test_e = np.load('ResNet18_{}_CSAGA_Adam_test_err.npy'.format(data),allow_pickle = True).tolist()\n",
    "\n",
    "model_SAGA_losses = np.load('ResNet18_{}_CSAGA_Adam_no_beta_running_loss.npy'.format(data),allow_pickle = True).tolist()\n",
    "model_SAGA_epoch_losses = np.load('ResNet18_{}_CSAGA_Adam_no_beta_epoch_loss.npy'.format(data),allow_pickle = True).tolist()\n",
    "SAGA_avg_var = np.load('ResNet18_{}_CSAGA_Adam_no_beta_running_var.npy'.format(data),allow_pickle = True).tolist()\n",
    "SAGA_epoch_var = np.load('ResNet18_{}_CSAGA_Adam_no_beta_epoch_var.npy'.format(data),allow_pickle = True).tolist()\n",
    "model_SAGA_test_e = np.load('ResNet18_{}_CSAGA_Adam_no_beta_test_err.npy'.format(data),allow_pickle = True).tolist()\n",
    "\n",
    "model_SAGA_2_losses = np.load('ResNet18_{}_CSAGA_Adam_2_running_loss.npy'.format(data),allow_pickle = True).tolist()\n",
    "model_SAGA_2_epoch_losses = np.load('ResNet18_{}_CSAGA_Adam_2_epoch_loss.npy'.format(data),allow_pickle = True).tolist()\n",
    "SAGA_2_avg_var = np.load('ResNet18_{}_CSAGA_Adam_2_running_var.npy'.format(data),allow_pickle = True).tolist()\n",
    "SAGA_2_epoch_var = np.load('ResNet18_{}_CSAGA_Adam_2_epoch_var.npy'.format(data),allow_pickle = True).tolist()\n",
    "model_SAGA_2_test_e = np.load('ResNet18_{}_CSAGA_Adam_2_test_err.npy'.format(data),allow_pickle = True).tolist()\n",
    "\n",
    "model_SVRG_losses = np.load('ResNet18_{}_SVRG_Adam_running_loss.npy'.format(data),allow_pickle = True).tolist()\n",
    "model_SVRG_epoch_losses = np.load('ResNet18_{}_SVRG_Adam_epoch_loss.npy'.format(data),allow_pickle = True).tolist()\n",
    "SVRG_avg_var = np.load('ResNet18_{}_SVRG_Adam_running_var.npy'.format(data),allow_pickle = True).tolist()\n",
    "SVRG_epoch_var = np.load('ResNet18_{}_SVRG_Adam_epoch_var.npy'.format(data),allow_pickle = True).tolist()\n",
    "model_SVRG_test_e = np.load('ResNet18_{}_SVRG_Adam_test_err.npy'.format(data),allow_pickle = True).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_val(losses, labels,val = \"loss\"):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    for loss, label in zip(losses, labels):\n",
    "        if len(loss) != 0:\n",
    "            print(label,loss[-1])\n",
    "        plt.plot(loss, label = label)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('ResNet cifar10 {} (class,{}) (lr: {}, n_samples = {}, betas: {})'.format(centered,\n",
    "                                                                                          opti,lr,N_SAMPLES,betas))\n",
    "    #plt.ylim(0,0.4)\n",
    "    #plt.yscale('log')\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#running average loss\n",
    "plot_val([model_SGD_losses,model_SAGA_losses,model_SAGA_2_losses,model_SVRG_losses],\n",
    "         ['SGD','SAGA (gradient momentum ({}))'.format(MOMENTUM_SAGA),'SAGA 2pc','SVRG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#average loss per epoch\n",
    "plot_val([model_SGD_epoch_losses,model_SAGA_epoch_losses,model_SAGA_2_epoch_losses,model_SVRG_epoch_losses],\n",
    "         ['SGD','SAGA (gradient momentum ({}))'.format(MOMENTUM_SAGA),'SAGA 2pc','SVRG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_val(losses, labels,val = \"loss\"):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    for losses, label in zip(losses, labels):\n",
    "        plt.plot(losses, label = label)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('ResNet cifar10 {} (class,{}) (lr: {}, n_samples = {}, betas: {})'.format(centered,\n",
    "                                                                                          opti,lr,N_SAMPLES,betas))\n",
    "    plt.xlabel('iteration')\n",
    "    #plt.ylim(0,100)\n",
    "    plt.ylabel(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#running var\n",
    "plot_val([SGD_avg_var,SAGA_avg_var,SAGA_2_avg_var,SVRG_avg_var],\n",
    "         ['SGD var','SAGA var (gradient momentum ({}))'.format(MOMENTUM_SAGA),'SAGA 2pc','SVRG'],\"variance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#var at each epoch\n",
    "plot_val([SGD_epoch_var,SAGA_epoch_var,SAGA_2_epoch_var,SVRG_epoch_var],\n",
    "         ['SGD var','SAGA var (gradient momentum ({}))'.format(MOMENTUM_SAGA),'SAGA 2pc','SVRG'],\"variance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_val(losses, labels,val = \"loss\"):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    for loss, label in zip(losses, labels):\n",
    "        if len(loss) != 0:\n",
    "            print(label,loss[-1])\n",
    "        plt.plot(loss, label = label)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('ResNet cifar10 {} (class,{}) (lr: {}, n_samples = {}, betas: {})'.format(centered,\n",
    "                                                                                          opti,lr,N_SAMPLES,betas))\n",
    "    plt.xlabel('iteration')\n",
    "    #plt.ylim(0.4,1.1)\n",
    "    plt.ylabel(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.array(SAGA_avg_var)\n",
    "t = np.array(SGD_avg_var)\n",
    "s2 = np.array(SAGA_2_avg_var)\n",
    "ratio1 = s/t\n",
    "#ratio2 = s2/t\n",
    "#type(s[0])\n",
    "plot_val([ratio1],[\"ratio saga\"],val = \"var ratio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_err(losses, labels,val = \"Error\"):\n",
    "    plt.figure(figsize=(6,6))\n",
    "    for losses, label in zip(losses, labels):\n",
    "        plt.plot(losses, label = label)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlabel('Iteration (k)',fontsize = 25)\n",
    "    plt.xticks(fontsize=13)\n",
    "    plt.yticks(fontsize=13)\n",
    "    plt.ylabel(val,fontsize = 25)\n",
    "    #plt.ylim(50, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_err([model_SGD_test_e,model_SAGA_test_e,model_SAGA_2_test_e,model_SVRG_test_e],\n",
    "         ['SGD','SAGApc(1)','SAGApc(2)','SVRG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchsummary import summary\n",
    "# summary(model_SAGA, input_size=(3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
